{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PihcyjYpS6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc3b6e66-2923-4ca7-887a-fed6ca3a921f"
      },
      "source": [
        "!pip3 install gym-retro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-retro\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6d/2c9f009663b74bcf66a2306c2b8a819a1ac6b0d3090e342720291b527446/gym_retro-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (162.0MB)\n",
            "\u001b[K     |████████████████████████████████| 162.0MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.17.1)\n",
            "Requirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.18.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Installing collected packages: gym-retro\n",
            "Successfully installed gym-retro-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny1sMLOZ3zoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "e1f009b0-fb19-49c5-e209-d6abc999ed85"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDdnC90spYXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "0896f556-09ec-46da-8cb3-fb5714188f95"
      },
      "source": [
        "!python -m retro.import /content/gdrive/My\\ Drive/Colab\\ Notebooks/aps360/project/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing FerrariGrandPrixChallenge-Genesis\n",
            "Imported 1 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQFgrJMWpUwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import retro\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchvision.models\n",
        "import gym\n",
        "import torch.nn.functional as F\n",
        "from math import floor\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XK_vn9HlKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIs8-EmrpWhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discretizer(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    Wrap a gym environment and make it use discrete actions.\n",
        "    Args:\n",
        "        combos: ordered list of lists of valid button combinations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, combos):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
        "        buttons = env.unwrapped.buttons\n",
        "        print(buttons)\n",
        "        self._decode_discrete_action = []\n",
        "        for combo in combos:\n",
        "            arr = np.array([False] * env.action_space.n)\n",
        "            for button in combo:\n",
        "                arr[buttons.index(button)] = True\n",
        "            self._decode_discrete_action.append(arr)\n",
        "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self._decode_discrete_action[act].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwM8s6hcpg7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, env, lr, kernel_sizes = [2, 2, 2]):\n",
        "        super(Policy, self).__init__()\n",
        "        self.process_image = nn.Sequential(\n",
        "            nn.Conv2d(3, 10, kernel_size=kernel_sizes[0]),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(10, 17, kernel_size=kernel_sizes[1]),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(17),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(17, 25, kernel_size=kernel_sizes[2]),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(25),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.n_outputs = env.action_space.n\n",
        "        self.input_size = 25*self.size_after_conv(224, kernel_sizes)*self.size_after_conv(320, kernel_sizes)\n",
        "\n",
        "        self.pick_action = nn.Sequential(\n",
        "            nn.Linear(self.input_size, 40),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),           \n",
        "            nn.Linear(40, self.n_outputs)\n",
        "        )\n",
        "        \n",
        "        self.lr = lr\n",
        "        self.optimizer = optim.Adam(self.parameters(), self.lr)\n",
        "\n",
        "    def size_after_conv(self, x, kernel_sizes):\n",
        "        s = floor((x - kernel_sizes[0] + 1)/2)\n",
        "        s = floor((s - kernel_sizes[1] + 1)/2)\n",
        "        s = floor((s - kernel_sizes[2] + 1)/2)\n",
        "        return s\n",
        "            \n",
        "    def forward(self, x):\n",
        "        out = self.process_image(x)\n",
        "        out = out.view(-1, self.input_size)\n",
        "        out = self.pick_action(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvomJKt2CznA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, env, lr = 0.01, gamma=0.99):\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        # self.i=0\n",
        "        self.reward_memory = []\n",
        "        self.action_memory = []\n",
        "        self.policy = Policy(env, lr)\n",
        "        self.policy.cuda()\n",
        "        #self.policy.load_state_dict(torch.load(\"/content/gdrive/My Drive/Colab Notebooks/aps360/project/Model/19/model_{}\".format(41)))\n",
        "\n",
        "    def choose_action(self, state, ep):\n",
        "        if (ep%2!=0):\n",
        "            probs = F.softmax(self.policy.forward(state), dim=1)\n",
        "            action_probs = torch.distributions.Categorical(probs)\n",
        "            action = action_probs.sample()\n",
        "            log_probs = action_probs.log_prob(action)\n",
        "            # self.i=0\n",
        "        else:\n",
        "            probs = (1/4)*np.ones(4)\n",
        "            probs=torch.tensor(probs, device=\"cuda:0\", requires_grad=True)\n",
        "            action_probs = torch.distributions.Categorical(probs)\n",
        "            action = action_probs.sample()\n",
        "            log_probs = action_probs.log_prob(action) \n",
        "        #     self.i+=1         \n",
        "        self.action_memory.append(log_probs)\n",
        "        return action.item()\n",
        "\n",
        "    def remember_rewards(self, reward):\n",
        "        self.reward_memory.append(reward)\n",
        "\n",
        "    def discount_rewards(self):\n",
        "        discounted_rewards = np.zeros_like(self.reward_memory, dtype=np.float64)\n",
        "        for t in range(len(self.reward_memory)):\n",
        "            G_t = 0\n",
        "            discount = 1\n",
        "            for j in range(t, len(self.reward_memory)):\n",
        "                G_t += self.reward_memory[j]*discount\n",
        "                discount *= self.gamma\n",
        "\n",
        "            discounted_rewards[t] = G_t\n",
        "        \n",
        "        mean = np.mean(discounted_rewards)\n",
        "        std_dev = np.std(discounted_rewards) if np.std(discounted_rewards) > 0 else 1\n",
        "        discounted_rewards = (discounted_rewards - mean)/std_dev\n",
        "        return discounted_rewards\n",
        "\n",
        "    def learn(self):\n",
        "        \n",
        "        \n",
        "        rewards = torch.Tensor(self.discount_rewards()).cuda()\n",
        "        \n",
        "        loss = 0\n",
        "        for r, logprob in zip(rewards, self.action_memory):\n",
        "            loss -= r*logprob\n",
        "        self.policy.optimizer.zero_grad() # zero out the gradient from last time\n",
        "        # print(\"loss = \", loss)\n",
        "        loss.backward()\n",
        "        self.policy.optimizer.step()\n",
        "\n",
        "        self.action_memory.clear()\n",
        "        self.reward_memory.clear()       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvOHZoSEQLL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    env = retro.make(game='FerrariGrandPrixChallenge-Genesis', record='.')\n",
        "    env = Discretizer(env, combos=[['A'], ['B'], ['LEFT', 'A'], ['RIGHT', 'A']])\n",
        "    #env = Discretizer(env, combos=[['A'], ['LEFT', 'A'], ['RIGHT', 'A']])\n",
        "    agent = Agent(env, lr = 0.001, gamma=0.55)\n",
        "    \n",
        "\n",
        "    score_history = []\n",
        "    score = 0\n",
        "    n_episodes = 200\n",
        "\n",
        "    for i in range(0, n_episodes):\n",
        "        print(\"Episode: {}, Score: {}\".format(i + 1, score))\n",
        "        done = False\n",
        "        score = 0\n",
        "        prev_observation = env.reset()\n",
        "        prev_observation = prev_observation.transpose((2,0,1))\n",
        "        new_observation, prev_speed, _, _ = env.step(0)\n",
        "        new_observation = new_observation.transpose((2,0,1))\n",
        "\n",
        "        state = new_observation - prev_observation\n",
        "        state = torch.Tensor(state).cuda().unsqueeze_(0)\n",
        "        num_steps = 0\n",
        "        actions=[]\n",
        "\n",
        "        while not done:\n",
        "            torch.cuda.empty_cache()\n",
        "            prev_observation = new_observation\n",
        "\n",
        "            # if (i==0):\n",
        "            #   prob = (1/4)*np.ones(4)\n",
        "            #   action = np.random.choice([0,1,2,3], p=prob)\n",
        "            # else:\n",
        "            action = agent.choose_action(state, i)\n",
        "            actions.append(action)\n",
        "            new_observation, speed, done, info = env.step(action)\n",
        "            num_steps += 1\n",
        "\n",
        "            new_observation = new_observation.transpose((2,0,1))\n",
        "            state = new_observation - prev_observation\n",
        "            # plt.imshow(state.transpose(1,2,0))\n",
        "            # plt.show()\n",
        "            state = torch.Tensor(state).cuda().unsqueeze_(0)\n",
        "\n",
        "            # if (new_observation[0,215,95] != 96 and new_observation[1,215,95] != 100) or (new_observation[0,215,205] != 96 and new_observation[1,215,205] != 100): \n",
        "            #     reward = 0\n",
        "            # elif reward < prev_speed:\n",
        "            #     reward = 0\n",
        "            # if (speed>250):\n",
        "            #       reward=3\n",
        "            # if (speed>200):\n",
        "            #      reward=3\n",
        "            if (speed>150):\n",
        "                 reward=2\n",
        "            elif (speed>100):\n",
        "                 reward=1\n",
        "            elif(speed>50):\n",
        "                  reward=1\n",
        "            else:\n",
        "                reward=0\n",
        "            \n",
        "\n",
        "            prev_speed = reward\n",
        "\n",
        "            agent.remember_rewards(reward)\n",
        "            score += reward\n",
        "\n",
        "            if num_steps > 0 and num_steps % 500 == 0:\n",
        "                print(info)\n",
        "                print(\"Episode: {}, Num steps: {}, Action: {}, Reward: {}\".format(i + 1, num_steps, action, reward))\n",
        "                print(score)\n",
        "\n",
        "            if num_steps > 0 and num_steps % 1000 == 0:\n",
        "                agent.learn()\n",
        "\n",
        "        # if (i%2!=0):\n",
        "        #     plt.plot(range(1,i+1), score_history)\n",
        "        #     plt.title(\"Reward as a function of the number of epochs\")\n",
        "        #     plt.ylabel(\"Reward\")\n",
        "        #     plt.xlabel(\"epoch\")\n",
        "        #     plt.show()            \n",
        "        plt.hist(actions, bins=4)\n",
        "        plt.show()\n",
        "        score_history.append(score)\n",
        "        agent.learn()\n",
        "        torch.save(agent.policy.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/aps360/project/Model/21/model_{}\".format(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zwIGebQVSU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c2797d0-2ef2-4470-a136-559a04bedf09"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B', 'A', 'MODE', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'C', 'Y', 'X', 'Z']\n",
            "Episode: 1, Score: 0\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 15, 'time': 689}\n",
            "Episode: 1, Num steps: 500, Action: 3, Reward: 0\n",
            "0\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 12, 'time': 1439}\n",
            "Episode: 1, Num steps: 1000, Action: 1, Reward: 0\n",
            "112\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 12, 'time': 2189}\n",
            "Episode: 1, Num steps: 1500, Action: 2, Reward: 0\n",
            "324\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 56, 'time': 2939}\n",
            "Episode: 1, Num steps: 2000, Action: 1, Reward: 1\n",
            "376\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 91, 'time': 3689}\n",
            "Episode: 1, Num steps: 2500, Action: 0, Reward: 1\n",
            "876\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 28, 'time': 4439}\n",
            "Episode: 1, Num steps: 3000, Action: 3, Reward: 0\n",
            "876\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 12, 'time': 5189}\n",
            "Episode: 1, Num steps: 3500, Action: 2, Reward: 0\n",
            "876\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 10, 'time': 5939}\n",
            "Episode: 1, Num steps: 4000, Action: 3, Reward: 0\n",
            "876\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 14, 'time': 10689}\n",
            "Episode: 1, Num steps: 4500, Action: 2, Reward: 0\n",
            "1216\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 11, 'time': 11439}\n",
            "Episode: 1, Num steps: 5000, Action: 0, Reward: 0\n",
            "1300\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 109, 'time': 12189}\n",
            "Episode: 1, Num steps: 5500, Action: 0, Reward: 1\n",
            "1408\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 74, 'time': 12939}\n",
            "Episode: 1, Num steps: 6000, Action: 1, Reward: 1\n",
            "1776\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 11, 'time': 13689}\n",
            "Episode: 1, Num steps: 6500, Action: 1, Reward: 0\n",
            "1884\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 13, 'time': 14439}\n",
            "Episode: 1, Num steps: 7000, Action: 2, Reward: 0\n",
            "1884\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 34, 'time': 15189}\n",
            "Episode: 1, Num steps: 7500, Action: 1, Reward: 0\n",
            "1884\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 0, 'time': 15939}\n",
            "Episode: 1, Num steps: 8000, Action: 1, Reward: 0\n",
            "1884\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 89, 'time': 20689}\n",
            "Episode: 1, Num steps: 8500, Action: 0, Reward: 1\n",
            "2160\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 42, 'time': 601}\n",
            "Episode: 1, Num steps: 9000, Action: 3, Reward: 0\n",
            "2504\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 8, 'time': 1351}\n",
            "Episode: 1, Num steps: 9500, Action: 0, Reward: 0\n",
            "2592\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 21, 'time': 2101}\n",
            "Episode: 1, Num steps: 10000, Action: 3, Reward: 0\n",
            "2708\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 95, 'time': 2851}\n",
            "Episode: 1, Num steps: 10500, Action: 3, Reward: 1\n",
            "2800\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 76, 'time': 3601}\n",
            "Episode: 1, Num steps: 11000, Action: 1, Reward: 1\n",
            "3032\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 18, 'time': 4351}\n",
            "Episode: 1, Num steps: 11500, Action: 3, Reward: 0\n",
            "3436\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 51, 'time': 5101}\n",
            "Episode: 1, Num steps: 12000, Action: 0, Reward: 1\n",
            "3440\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 4, 'time': 5851}\n",
            "Episode: 1, Num steps: 12500, Action: 0, Reward: 0\n",
            "3836\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 50, 'time': 10601}\n",
            "Episode: 1, Num steps: 13000, Action: 1, Reward: 0\n",
            "3888\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 56, 'time': 11351}\n",
            "Episode: 1, Num steps: 13500, Action: 1, Reward: 1\n",
            "4076\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 46, 'time': 12101}\n",
            "Episode: 1, Num steps: 14000, Action: 0, Reward: 0\n",
            "4240\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 34, 'time': 12851}\n",
            "Episode: 1, Num steps: 14500, Action: 2, Reward: 0\n",
            "4604\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 70, 'time': 13601}\n",
            "Episode: 1, Num steps: 15000, Action: 1, Reward: 1\n",
            "4616\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 21, 'time': 14351}\n",
            "Episode: 1, Num steps: 15500, Action: 1, Reward: 0\n",
            "4644\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 5, 'time': 15101}\n",
            "Episode: 1, Num steps: 16000, Action: 3, Reward: 0\n",
            "4660\n",
            "{'best_lap': 20838, 'lap': 2, 'speed': 38, 'time': 15851}\n",
            "Episode: 1, Num steps: 16500, Action: 0, Reward: 0\n",
            "4680\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 78, 'time': 265}\n",
            "Episode: 1, Num steps: 17000, Action: 3, Reward: 1\n",
            "5124\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 68, 'time': 1015}\n",
            "Episode: 1, Num steps: 17500, Action: 0, Reward: 1\n",
            "5624\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 26, 'time': 1765}\n",
            "Episode: 1, Num steps: 18000, Action: 0, Reward: 0\n",
            "5756\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 8, 'time': 2515}\n",
            "Episode: 1, Num steps: 18500, Action: 3, Reward: 0\n",
            "5756\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 40, 'time': 3265}\n",
            "Episode: 1, Num steps: 19000, Action: 3, Reward: 0\n",
            "5848\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 12, 'time': 4015}\n",
            "Episode: 1, Num steps: 19500, Action: 1, Reward: 0\n",
            "5848\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 37, 'time': 4765}\n",
            "Episode: 1, Num steps: 20000, Action: 0, Reward: 0\n",
            "6148\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 50, 'time': 5515}\n",
            "Episode: 1, Num steps: 20500, Action: 0, Reward: 0\n",
            "6148\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 3, 'time': 10265}\n",
            "Episode: 1, Num steps: 21000, Action: 0, Reward: 0\n",
            "6336\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 15, 'time': 11015}\n",
            "Episode: 1, Num steps: 21500, Action: 1, Reward: 0\n",
            "6336\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 24, 'time': 11765}\n",
            "Episode: 1, Num steps: 22000, Action: 1, Reward: 0\n",
            "6336\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 46, 'time': 12515}\n",
            "Episode: 1, Num steps: 22500, Action: 2, Reward: 0\n",
            "6336\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 27, 'time': 13265}\n",
            "Episode: 1, Num steps: 23000, Action: 3, Reward: 0\n",
            "6636\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 26, 'time': 14015}\n",
            "Episode: 1, Num steps: 23500, Action: 1, Reward: 0\n",
            "6808\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 22, 'time': 14765}\n",
            "Episode: 1, Num steps: 24000, Action: 3, Reward: 0\n",
            "6924\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 36, 'time': 15515}\n",
            "Episode: 1, Num steps: 24500, Action: 2, Reward: 0\n",
            "7056\n",
            "{'best_lap': 20336, 'lap': 3, 'speed': 33, 'time': 20265}\n",
            "Episode: 1, Num steps: 25000, Action: 2, Reward: 0\n",
            "7056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 2, Score: 7131\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 23, 'time': 689}\n",
            "Episode: 2, Num steps: 500, Action: 0, Reward: 0\n",
            "164\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 52, 'time': 1439}\n",
            "Episode: 2, Num steps: 1000, Action: 0, Reward: 1\n",
            "172\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 9, 'time': 2189}\n",
            "Episode: 2, Num steps: 1500, Action: 0, Reward: 0\n",
            "264\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 11, 'time': 2939}\n",
            "Episode: 2, Num steps: 2000, Action: 2, Reward: 0\n",
            "264\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 12, 'time': 3689}\n",
            "Episode: 2, Num steps: 2500, Action: 1, Reward: 0\n",
            "264\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 11, 'time': 4439}\n",
            "Episode: 2, Num steps: 3000, Action: 0, Reward: 0\n",
            "264\n",
            "{'best_lap': 595999, 'lap': 1, 'speed': 18, 'time': 5189}\n",
            "Episode: 2, Num steps: 3500, Action: 1, Reward: 0\n",
            "264\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}