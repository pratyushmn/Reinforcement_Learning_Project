{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C with PPO and GAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dacdcdf5a696470f8a6cbf7f96c87052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_09a6ca6f6a384f3bb9ba56d9fd95972f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f39d1ea74b664ab8b181075201071fa9",
              "IPY_MODEL_9fc3f951f89a4ca292ff97fd026b3e99"
            ]
          }
        },
        "09a6ca6f6a384f3bb9ba56d9fd95972f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f39d1ea74b664ab8b181075201071fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb94403bbeb94ec495e9f87ce691e141",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 138223492,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 138223492,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92864beef4034880b7a9cc6247ace199"
          }
        },
        "9fc3f951f89a4ca292ff97fd026b3e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4f689e6d60a4bd187199af3e09dda73",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 132M/132M [00:02&lt;00:00, 64.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21518dc493b94cc08e611070ea2e8c18"
          }
        },
        "bb94403bbeb94ec495e9f87ce691e141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92864beef4034880b7a9cc6247ace199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4f689e6d60a4bd187199af3e09dda73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21518dc493b94cc08e611070ea2e8c18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kjeNHGWecKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "5664e8a0-03b8-4ff1-df15-b63e8257d089"
      },
      "source": [
        "!pip3 install gym-retro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-retro\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6d/2c9f009663b74bcf66a2306c2b8a819a1ac6b0d3090e342720291b527446/gym_retro-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (162.0MB)\n",
            "\u001b[K     |████████████████████████████████| 162.0MB 99kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.17.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.18.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.12.0)\n",
            "Installing collected packages: gym-retro\n",
            "Successfully installed gym-retro-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zyV0-Hbelgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "28c32a13-e860-4d34-d4f2-c012603cf0b1"
      },
      "source": [
        "!python3 -m retro.import /content/sample_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing FerrariGrandPrixChallenge-Genesis\n",
            "Imported 1 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oVHwRsYgiZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "9a6b3a27-9056-4564-8d25-afd944712450"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mljUujhfenKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import retro\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchvision.models\n",
        "import torchvision\n",
        "import gym\n",
        "import torch.nn.functional as F\n",
        "from math import floor\n",
        "import time\n",
        "from torch.distributions import Categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPTv2e9OUdX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 10\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yToUdFFTeuz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE       = 1e-3\n",
        "GAMMA               = 0.9 # discount factor for future rewards\n",
        "LAMBDA              = 0.95 # lambda used in advantage estimation\n",
        "PPO_EPSILON         = 0.2 # to prevent large changes to the policy\n",
        "CRITIC_DISCOUNT     = 0.5 # reducing the weight of the critic error when computing total network loss\n",
        "ENTROPY_BETA        = 0.01 # factor to reward exploration over exploitation\n",
        "MINI_BATCH_SIZE     = 256 \n",
        "PPO_EPOCHS          = 3 # number of times to update weights using the same data\n",
        "HIDDEN_LAYERS       = [256, 32] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwlnsT9LfJdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discretizer(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    Wrap a gym environment and make it use discrete actions.\n",
        "    Args:\n",
        "        combos: ordered list of lists of valid button combinations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, combos):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
        "        buttons = env.unwrapped.buttons\n",
        "        self._decode_discrete_action = []\n",
        "        for combo in combos:\n",
        "            arr = np.array([False] * env.action_space.n)\n",
        "            for button in combo:\n",
        "                arr[buttons.index(button)] = True\n",
        "            self._decode_discrete_action.append(arr)\n",
        "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self._decode_discrete_action[act].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hL1NwMafRTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_actions, hidden = HIDDEN_LAYERS):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.input_size = 2048 # for wide resnet\n",
        "        self.n_outputs = num_actions\n",
        "        self.pick_action = nn.Sequential(\n",
        "            nn.Linear(self.input_size, hidden[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden[0], hidden[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden[1], self.n_outputs)\n",
        "        )\n",
        "        self.evaluate = nn.Sequential(\n",
        "            nn.Linear(self.input_size, hidden[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden[0], hidden[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden[1], 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_size)\n",
        "        return self.pick_action(x), self.evaluate(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-WI6iR2pe_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "dacdcdf5a696470f8a6cbf7f96c87052",
            "09a6ca6f6a384f3bb9ba56d9fd95972f",
            "f39d1ea74b664ab8b181075201071fa9",
            "9fc3f951f89a4ca292ff97fd026b3e99",
            "bb94403bbeb94ec495e9f87ce691e141",
            "92864beef4034880b7a9cc6247ace199",
            "a4f689e6d60a4bd187199af3e09dda73",
            "21518dc493b94cc08e611070ea2e8c18"
          ]
        },
        "outputId": "0b1f1997-8bbc-4626-c294-f6fe5a728482"
      },
      "source": [
        "# Using the pretrained Wide Resnet 50 model to extract features\n",
        "resnet = torchvision.models.wide_resnet50_2(pretrained=True)\n",
        "\n",
        "# Keeping all but the last (fully-connected) layer from the pretrained resnet model\n",
        "modules=list(resnet.children())[:-1]\n",
        "resnet=nn.Sequential(*modules)\n",
        "\n",
        "# freezing the parameters so they don't get changed during training\n",
        "for p in resnet.parameters():\n",
        "    p.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\" to /root/.cache/torch/checkpoints/wide_resnet50_2-95faca4d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dacdcdf5a696470f8a6cbf7f96c87052",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=138223492), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XkuP-OZpXwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_image(image):\n",
        "    # transforming the image so that it's in the form that the pretrained resnet model expects\n",
        "    mean=[0.485, 0.456, 0.406]\n",
        "    std=[0.229, 0.224, 0.225]\n",
        "    new_img = image.transpose((2,0,1))\n",
        "    new_img = (new_img-new_img.min())/(new_img.max() - new_img.min())\n",
        "    new_img = torch.Tensor(new_img).to(device)    \n",
        "    new_img = torchvision.transforms.functional.normalize(new_img, mean, std, inplace=True)\n",
        "\n",
        "    # extracting and returning the features\n",
        "    out = resnet(new_img.unsqueeze_(0))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjU_Z7CAgT4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x):\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-8)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVtt2nMigWj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generalized Advantage Estimation is the state-of-the-art method to determine how \"good\" the choice of action is\n",
        "def GAE(next_value, rewards, masks, values):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        #  formula from the GAE paper\n",
        "        delta = rewards[step] + GAMMA * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + GAMMA * LAMBDA * masks[step] * gae\n",
        "        \n",
        "        # insert at the beginning to get correct order back\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JfyGfEhgbE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate(states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    # generates random mini-batches from the collected data until the entire batch has been covered\n",
        "    for _ in range(batch_size//MINI_BATCH_SIZE):\n",
        "        rand_ids = np.random.randint(0, batch_size, MINI_BATCH_SIZE)\n",
        "        yield states[rand_ids, :], actions[rand_ids], log_probs[rand_ids], returns[rand_ids], advantage[rand_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsbyW00-gfZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(states, actions, log_probs, returns, advantages):\n",
        "    # PPO EPOCHS is the number of times we go through all the collected data to make updates\n",
        "    for _ in range(PPO_EPOCHS):\n",
        "        # grabs random mini-batches several times until we have covered all data\n",
        "        for state, action, old_log_probs, return_, advantage in iterate(states, actions, log_probs, returns, advantages):\n",
        "            act, value = model(state)\n",
        "            dist = Categorical(F.softmax(act, dim=-1))\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            prob_ratio = (new_log_probs - old_log_probs).exp()\n",
        "            \n",
        "            # PPO loss for the actor\n",
        "            actor_loss  = - torch.min(prob_ratio * advantage, torch.clamp(prob_ratio, 1.0 - PPO_EPSILON, 1.0 + PPO_EPSILON) * advantage).mean()\n",
        "            \n",
        "            #  Regular MSE loss for the critic\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            # total loss for the network is the critic loss weighted by a decreasing factor + actor loss \n",
        "            # + entropy weighted by a factor (the entropy is there to encourage exploration - ie. not converging to one prediction too soon)\n",
        "            loss = CRITIC_DISCOUNT * critic_loss + actor_loss - ENTROPY_BETA * entropy\n",
        "\n",
        "            # optimize parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfrutzs9gpAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "493881c6-75c1-4419-fd05-9a659c467e3c"
      },
      "source": [
        "# try with using a CNN instead of the resnet\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Device:', device)\n",
        "\n",
        "env = retro.make(game='FerrariGrandPrixChallenge-Genesis')\n",
        "env = Discretizer(env, combos=[['A'], ['B'], ['LEFT', 'A'], ['RIGHT', 'A']])\n",
        "num_outputs = env.action_space.n\n",
        "\n",
        "model = ActorCritic(num_outputs)\n",
        "# model.load_state_dict(torch.load(\"/content/gdrive/My Drive/APS360 Project/PPO Models/PPO_A2C_39_1100225.0_-0.005999565124511719\"))\n",
        "model.to(device)\n",
        "resnet.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "best_reward = None\n",
        "best_advantage = None\n",
        "\n",
        "# all_rewards = np.load(\"/content/gdrive/My Drive/APS360 Project/PPO Models/all_rewards.npy\")\n",
        "all_rewards = np.array([])\n",
        "total_iters = len(all_rewards)\n",
        "\n",
        "while True:\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    values    = []\n",
        "    states    = []\n",
        "    actions   = []\n",
        "    rewards   = []\n",
        "    masks     = []\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    total_adv = 0\n",
        "    prevSpeed = 0\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while not done:           \n",
        "        state = transform_image(state)\n",
        "\n",
        "        act, value = model(state)\n",
        "\n",
        "        if total_iters %2 == 1:\n",
        "            dist = Categorical(F.softmax(act, dim=-1))\n",
        "        else:\n",
        "            probs = (1/4)*np.ones(4)\n",
        "            probs = torch.tensor(probs, device=\"cuda:0\", requires_grad=True)\n",
        "            dist = Categorical(probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        next_state, reward, done, info = env.step(action.item())\n",
        "        \n",
        "        speed = info['speed'] \n",
        "        screen = env.get_screen()\n",
        "\n",
        "        if speed > 200:\n",
        "            reward = torch.tensor([4], dtype=torch.long, device=device)\n",
        "        elif speed > 150:\n",
        "            reward = torch.tensor([3], dtype=torch.long, device=device)\n",
        "        elif speed > 100:\n",
        "            reward = torch.tensor([2], dtype=torch.long, device=device)\n",
        "        elif speed > 75:\n",
        "            reward = torch.tensor([1], dtype=torch.long, device=device)\n",
        "        elif speed > 40:\n",
        "            reward = torch.tensor([0], dtype=torch.long, device=device)\n",
        "        else:\n",
        "            reward = torch.tensor([-2], dtype=torch.long, device=device)\n",
        "                \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.as_tensor(reward).unsqueeze(-1).to(device))\n",
        "        masks.append(torch.as_tensor(1 - done).unsqueeze(-1).to(device))\n",
        "        \n",
        "        state = next_state\n",
        "        prevSpeed = speed\n",
        "\n",
        "        if i % 2000 == 0:\n",
        "            print(\"i: {}, Action: {}, Reward: {}, Speed: {}\".format(i, action.item(), reward, speed))\n",
        "\n",
        "        i += 1\n",
        "              \n",
        "    next_state = transform_image(next_state)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = GAE(next_value, rewards, masks, values)\n",
        "    returns = torch.stack(returns).detach()\n",
        "    log_probs = torch.stack(log_probs).detach()\n",
        "    values    = torch.stack(values).detach()\n",
        "    states    = torch.stack(states)\n",
        "    actions   = torch.stack(actions)\n",
        "    advantage = returns - values\n",
        "    advantage = normalize(advantage)\n",
        "\n",
        "    train(states, actions, log_probs, returns, advantage)\n",
        "\n",
        "    total_rewards += sum(rewards)\n",
        "    total_adv += sum(advantage)\n",
        "    total_iters += 1\n",
        "    \n",
        "    print(\"Iteration: {}, Reward = {}, Advantage = {}\".format(total_iters, total_rewards.item(), total_adv.item()))\n",
        "    print(info)\n",
        "\n",
        "    all_rewards = np.append(all_rewards, total_rewards.item())\n",
        "    np.save(\"/content/gdrive/My Drive/APS360 Project/PPO Models/all_rewards.npy\", all_rewards)\n",
        "    \n",
        "    if best_reward is None or best_reward < total_rewards or best_advantage is None or best_advantage < total_adv:\n",
        "        best_reward = total_rewards\n",
        "        best_advantage = total_adv\n",
        "        torch.save(model.state_dict(), \"/content/gdrive/My Drive/APS360 Project/PPO Models/PPO_A2C_{}_{}_{}\".format(total_iters, total_rewards.item(), total_adv.item()))\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "i: 0, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 81\n",
            "i: 4000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 67\n",
            "i: 6000, Action: 2, Reward: tensor([0], device='cuda:0'), Speed: 75\n",
            "i: 8000, Action: 3, Reward: tensor([1], device='cuda:0'), Speed: 94\n",
            "i: 10000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 9\n",
            "i: 12000, Action: 3, Reward: tensor([0], device='cuda:0'), Speed: 41\n",
            "i: 14000, Action: 0, Reward: tensor([1], device='cuda:0'), Speed: 92\n",
            "i: 16000, Action: 0, Reward: tensor([1], device='cuda:0'), Speed: 90\n",
            "i: 18000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 23\n",
            "i: 20000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 8\n",
            "i: 22000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 9\n",
            "i: 24000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 22\n",
            "Iteration: 1, Reward = -24508, Advantage = 0.006847381591796875\n",
            "{'best_lap': 15958, 'lap': 4, 'speed': 30, 'time': 0}\n",
            "i: 0, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 58\n",
            "i: 4000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 42\n",
            "i: 6000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 27\n",
            "i: 8000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 44\n",
            "i: 10000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 22\n",
            "i: 12000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 35\n",
            "i: 14000, Action: 3, Reward: tensor([0], device='cuda:0'), Speed: 53\n",
            "i: 16000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 21\n",
            "i: 18000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 37\n",
            "i: 20000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 19\n",
            "i: 22000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 28\n",
            "i: 24000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 43\n",
            "Iteration: 2, Reward = -23367, Advantage = -0.00011515617370605469\n",
            "{'best_lap': 20294, 'lap': 4, 'speed': 85, 'time': 0}\n",
            "i: 0, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 77\n",
            "i: 4000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 9\n",
            "i: 6000, Action: 2, Reward: tensor([0], device='cuda:0'), Speed: 53\n",
            "i: 8000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 19\n",
            "i: 10000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 13\n",
            "i: 12000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 5\n",
            "i: 14000, Action: 3, Reward: tensor([0], device='cuda:0'), Speed: 75\n",
            "i: 16000, Action: 0, Reward: tensor([1], device='cuda:0'), Speed: 83\n",
            "i: 18000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 17\n",
            "i: 20000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 17\n",
            "i: 22000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 15\n",
            "i: 24000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 43\n",
            "Iteration: 3, Reward = -25972, Advantage = -0.000946342945098877\n",
            "{'best_lap': 20396, 'lap': 4, 'speed': 31, 'time': 0}\n",
            "i: 0, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 16\n",
            "i: 4000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 57\n",
            "i: 6000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 40\n",
            "i: 8000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 11\n",
            "i: 10000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 10\n",
            "i: 12000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 13\n",
            "i: 14000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 19\n",
            "i: 16000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 18\n",
            "i: 18000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 8\n",
            "i: 20000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 12\n",
            "i: 22000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 22\n",
            "i: 24000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 6\n",
            "Iteration: 4, Reward = -28316, Advantage = -0.0005882978439331055\n",
            "{'best_lap': 20228, 'lap': 4, 'speed': 13, 'time': 0}\n",
            "i: 0, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 3, Reward: tensor([0], device='cuda:0'), Speed: 56\n",
            "i: 4000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 7\n",
            "i: 6000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 73\n",
            "i: 8000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 23\n",
            "i: 10000, Action: 2, Reward: tensor([0], device='cuda:0'), Speed: 51\n",
            "i: 12000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 98\n",
            "i: 14000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 53\n",
            "i: 16000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 24\n",
            "i: 18000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 30\n",
            "i: 20000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 37\n",
            "i: 22000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 21\n",
            "i: 24000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 8\n",
            "Iteration: 5, Reward = -23990, Advantage = -0.0018017292022705078\n",
            "{'best_lap': 20246, 'lap': 4, 'speed': 44, 'time': 0}\n",
            "i: 0, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 28\n",
            "i: 4000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 10\n",
            "i: 6000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 47\n",
            "i: 8000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 17\n",
            "i: 10000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 21\n",
            "i: 12000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 31\n",
            "i: 14000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 35\n",
            "i: 16000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 26\n",
            "i: 18000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 34\n",
            "i: 20000, Action: 1, Reward: tensor([1], device='cuda:0'), Speed: 91\n",
            "i: 22000, Action: 2, Reward: tensor([0], device='cuda:0'), Speed: 56\n",
            "i: 24000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 10\n",
            "Iteration: 6, Reward = -25039, Advantage = 0.006307840347290039\n",
            "{'best_lap': 20252, 'lap': 4, 'speed': 84, 'time': 0}\n",
            "i: 0, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 2, Reward: tensor([2], device='cuda:0'), Speed: 102\n",
            "i: 4000, Action: 1, Reward: tensor([1], device='cuda:0'), Speed: 83\n",
            "i: 6000, Action: 3, Reward: tensor([1], device='cuda:0'), Speed: 79\n",
            "i: 8000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 18\n",
            "i: 10000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 93\n",
            "i: 12000, Action: 2, Reward: tensor([0], device='cuda:0'), Speed: 64\n",
            "i: 14000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 10\n",
            "i: 16000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 11\n",
            "i: 18000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 73\n",
            "i: 20000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 14\n",
            "i: 22000, Action: 1, Reward: tensor([2], device='cuda:0'), Speed: 110\n",
            "i: 24000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 4\n",
            "Iteration: 7, Reward = -19516, Advantage = 0.004163265228271484\n",
            "{'best_lap': 20066, 'lap': 4, 'speed': 101, 'time': 0}\n",
            "i: 0, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 10\n",
            "i: 4000, Action: 2, Reward: tensor([0], device='cuda:0'), Speed: 61\n",
            "i: 6000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 10\n",
            "i: 8000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 11\n",
            "i: 10000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 27\n",
            "i: 12000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 10\n",
            "i: 14000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 12\n",
            "i: 16000, Action: 1, Reward: tensor([1], device='cuda:0'), Speed: 99\n",
            "i: 18000, Action: 0, Reward: tensor([1], device='cuda:0'), Speed: 81\n",
            "i: 20000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 20\n",
            "i: 22000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 25\n",
            "i: 24000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 9\n",
            "Iteration: 8, Reward = -26002, Advantage = 0.0005133152008056641\n",
            "{'best_lap': 20336, 'lap': 4, 'speed': 71, 'time': 0}\n",
            "i: 0, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 30\n",
            "i: 4000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 82\n",
            "i: 6000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 32\n",
            "i: 8000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 48\n",
            "i: 10000, Action: 3, Reward: tensor([1], device='cuda:0'), Speed: 100\n",
            "i: 12000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 94\n",
            "i: 14000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 14\n",
            "i: 16000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 29\n",
            "i: 18000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 35\n",
            "i: 20000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 36\n",
            "i: 22000, Action: 3, Reward: tensor([0], device='cuda:0'), Speed: 63\n",
            "i: 24000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 43\n",
            "Iteration: 9, Reward = -24367, Advantage = -0.008582115173339844\n",
            "{'best_lap': 20246, 'lap': 4, 'speed': 79, 'time': 0}\n",
            "i: 0, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 37\n",
            "i: 4000, Action: 1, Reward: tensor([1], device='cuda:0'), Speed: 92\n",
            "i: 6000, Action: 3, Reward: tensor([-2], device='cuda:0'), Speed: 28\n",
            "i: 8000, Action: 3, Reward: tensor([1], device='cuda:0'), Speed: 78\n",
            "i: 10000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 22\n",
            "i: 12000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 4\n",
            "i: 14000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 35\n",
            "i: 16000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 3\n",
            "i: 18000, Action: 3, Reward: tensor([0], device='cuda:0'), Speed: 57\n",
            "i: 20000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 12\n",
            "i: 22000, Action: 3, Reward: tensor([1], device='cuda:0'), Speed: 89\n",
            "i: 24000, Action: 2, Reward: tensor([0], device='cuda:0'), Speed: 55\n",
            "Iteration: 10, Reward = -23779, Advantage = 0.0030907392501831055\n",
            "{'best_lap': 20252, 'lap': 4, 'speed': 84, 'time': 0}\n",
            "i: 0, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 27\n",
            "i: 4000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 100\n",
            "i: 6000, Action: 0, Reward: tensor([1], device='cuda:0'), Speed: 85\n",
            "i: 8000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 24\n",
            "i: 10000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 65\n",
            "i: 12000, Action: 3, Reward: tensor([1], device='cuda:0'), Speed: 80\n",
            "i: 14000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 19\n",
            "i: 16000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 7\n",
            "i: 18000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 34\n",
            "i: 20000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 26\n",
            "i: 22000, Action: 3, Reward: tensor([2], device='cuda:0'), Speed: 109\n",
            "i: 24000, Action: 2, Reward: tensor([1], device='cuda:0'), Speed: 81\n",
            "Iteration: 11, Reward = -20772, Advantage = -0.0024300217628479004\n",
            "{'best_lap': 20258, 'lap': 4, 'speed': 36, 'time': 0}\n",
            "i: 0, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 0\n",
            "i: 2000, Action: 3, Reward: tensor([0], device='cuda:0'), Speed: 55\n",
            "i: 4000, Action: 2, Reward: tensor([-2], device='cuda:0'), Speed: 15\n",
            "i: 6000, Action: 0, Reward: tensor([-2], device='cuda:0'), Speed: 26\n",
            "i: 8000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 60\n",
            "i: 10000, Action: 0, Reward: tensor([0], device='cuda:0'), Speed: 66\n",
            "i: 12000, Action: 1, Reward: tensor([0], device='cuda:0'), Speed: 66\n",
            "i: 14000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 12\n",
            "i: 16000, Action: 1, Reward: tensor([-2], device='cuda:0'), Speed: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YYtSoM7_fGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwzVG_j4J7Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}